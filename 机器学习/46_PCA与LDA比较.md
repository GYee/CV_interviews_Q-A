## 问题

在前面总结的机器学习问题44和45中已经分别总结了 `PCA` 和 `LDA` 两种降维方法，接下来就对这两种方法一起做个对比和总结吧。

## PCA与LDA的比较

### 异同点

#### ● 相同点：

① 均是降维方法

② 降维时均使用了矩阵特征分解的思想

③ 两者都假设数据符合高斯分布

#### ● 不同点：

① PCA是无监督的降维方法，而LDA是有监督的降维方法

② LDA除了可以降维，还可以用于分类

③ LDA降维最多降到类别数 `k-1`的维数（k是样本类别的个数），而PCA没有这个限制。

④ LDA选择的是分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向

> ● 关于第3个点：
>
> 可能不太好理解，如果要搞清楚的话，可以参考文末的博客中 **第4小节-多类LDA原理** 中的最后一段。
>
> ● 关于第4点，可以这样理解：
>
> LDA在降维过程中最小化类内距离，即同类样本的方差尽可能小，同时最大化类间距离，即异类样本尽可能分离，这本身是也为分类任务服务的；而PCA是无监督的降维方法，其假设方差越大，信息量越多，因此会选择样本点投影具有最大方差的方向。



### 优缺点

#### ● LDA优点：

① 降维过程中可以使用类别的先验知识（有监督的），而PCA是无监督的

② 在样本分类信息依赖均值而不是方差的时候，LDA算法优于PCA算法     

#### ● LDA缺点：

① LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。

② LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。

③ LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。

④ LDA可能过度拟合数据。



#### ● PCA优点：

①它是无监督学习算法，完全无参数限制。

② 在样本分类信息依赖方差而不是均值的时候，PCA算法优于LDA算法     

#### ● PCA缺点：

①特征值分解有一些局限性，比如变换的矩阵必须是方阵

②如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高



## 参考资料

[线性判别分析LDA原理总结](https://www.cnblogs.com/pinard/p/6244265.html) https://www.cnblogs.com/pinard/p/6244265.html